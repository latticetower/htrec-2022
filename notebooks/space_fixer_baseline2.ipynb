{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seeds, make everything reproducible, etc (at least try to).\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "DATADIR = \"../data\"\n",
    "\n",
    "MAX_WORDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install aicrowd-cli\n",
    "%load_ext aicrowd.magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAPI Key valid\u001b[0m\n",
      "\u001b[33mGitlab oauth token invalid or absent.\n",
      "It is highly recommended to simply run `aicrowd login` without passing the API Key.\u001b[0m\n",
      "\u001b[32mSaved details successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%aicrowd login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c5a57020740e9947a5b77736a13a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/395k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d7e1d72e0446bbf45388d2c05cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/45.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "if Path(DATADIR).exists():\n",
    "  !rm -rf $DATADIR\n",
    "!mkdir $DATADIR\n",
    "%aicrowd ds dl -c htrec-2022 -o $DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUMAN_TRANSCRIPTION</th>\n",
       "      <th>SYSTEM_TRANSCRIPTION</th>\n",
       "      <th>CENTURY</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>TEXT_LINE_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει</td>\n",
       "      <td>ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...</td>\n",
       "      <td>του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ</td>\n",
       "      <td>τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>τῆς ἡδεῖας πλεονεξίας πολλοὺς εἰς τὴν τῶν ἀλλ</td>\n",
       "      <td>της ἐδίας πλσον ἐξιας πολλους ἐις τὴν τῶν ἀλ</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>οτρίων ἐπιθυμίαν προκαλούμενος ἐμπυρί</td>\n",
       "      <td>λοτρλων ἐπιθυμιαν προκαλουμένος ἐμπυρι</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 HUMAN_TRANSCRIPTION  \\\n",
       "0      ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει   \n",
       "1  τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...   \n",
       "2        τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ   \n",
       "3      τῆς ἡδεῖας πλεονεξίας πολλοὺς εἰς τὴν τῶν ἀλλ   \n",
       "4              οτρίων ἐπιθυμίαν προκαλούμενος ἐμπυρί   \n",
       "\n",
       "                              SYSTEM_TRANSCRIPTION  CENTURY  \\\n",
       "0         ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει       11   \n",
       "1  του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν       11   \n",
       "2           τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα       11   \n",
       "3     της ἐδίας πλσον ἐξιας πολλους ἐις τὴν τῶν ἀλ       11   \n",
       "4           λοτρλων ἐπιθυμιαν προκαλουμένος ἐμπυρι       11   \n",
       "\n",
       "                                          IMAGE_PATH  TEXT_LINE_NUM  \n",
       "0  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              1  \n",
       "1  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              2  \n",
       "2  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              3  \n",
       "3  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              4  \n",
       "4  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pywer\n",
    "train_df = pd.read_csv( f\"{DATADIR}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{DATADIR}/test.csv\")\n",
    "\n",
    "word_regex = re.compile(\"\\W+\")\n",
    "word_regex2 = re.compile(\"(\\W+)\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lib_dir in [\"..\", \"../src\"]:\n",
    "    if not lib_dir in sys.path:\n",
    "        sys.path.append(lib_dir)\n",
    "from lm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = make_lm(train_df.HUMAN_TRANSCRIPTION.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datastruct import *\n",
    "from common import *\n",
    "from space_fixer import SpaceFixer\n",
    "\n",
    "# for ht_line, mt_line in tqdm(train_df[[\"HUMAN_TRANSCRIPTION\", \"SYSTEM_TRANSCRIPTION\"]].values[7:]):\n",
    "#     ht_words = word_regex.split(ht_line)\n",
    "#     mt_words = word_regex.split(mt_line)\n",
    "#     #words_ = [remove_cap(word) for word in words]\n",
    "#     #vocab.add_sentence(words)\n",
    "#     dmatrix = build_path_matrix(mt_words, vocabs)\n",
    "#     finished_paths = extract_paths(dmatrix)\n",
    "#     for k in resplit_paths(finished_paths, mt_words):\n",
    "#         variant = \" \".join(k)\n",
    "#         print(variant)\n",
    "#     break\n",
    "CUTOFF = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spaces_dict(x):\n",
    "    result = dict()\n",
    "    last_index = 0\n",
    "    for w in word_regex2.split(x):\n",
    "        if not word_regex.match(w):\n",
    "            last_index += 1\n",
    "        else:\n",
    "            result[last_index] = w\n",
    "    if not last_index in result:\n",
    "        result[last_index] = \"\"\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c8846dd53c406da743d780d11baf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_df[\"SYSTEM_TRANSCRIPTION_raw\"] = train_df.SYSTEM_TRANSCRIPTION\n",
    "train_df[\"SYSTEM_TRANSCRIPTION\"] = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: lmr(x, lm=language_model))\n",
    "\n",
    "ht_sequences_train = train_df.HUMAN_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_sequences_train = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_spaces_train = train_df.SYSTEM_TRANSCRIPTION.apply(extract_spaces_dict).values\n",
    "\n",
    "mt_texts_train = train_df.SYSTEM_TRANSCRIPTION.values\n",
    "fixer = SpaceFixer(MAX_WORDS)\n",
    "fixer.fill(ht_sequences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Greek_Medieval_Corpus/Ποιητικά-Λογοτεχνικά/poe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ποιητικά-Λογοτεχνικά/poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Greek_Medieval_Corpus/Ποιητικά-Λογοτεχνικά/poe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ποιητικά-Λογοτεχνικά/poetry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename text  \\\n",
       "18  Greek_Medieval_Corpus/Ποιητικά-Λογοτεχνικά/poe...  NaN   \n",
       "25  Greek_Medieval_Corpus/Ποιητικά-Λογοτεχνικά/poe...  NaN   \n",
       "\n",
       "                          genre  \n",
       "18  Ποιητικά-Λογοτεχνικά/poetry  \n",
       "25  Ποιητικά-Λογοτεχνικά/poetry  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_corpus = pd.read_csv(\"../mgc.csv\")\n",
    "additional_corpus.head()\n",
    "additional_corpus.loc[additional_corpus.text.isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14c5f678855464692b583cfd1b8c690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44666 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sep_regex = re.compile(\"<[^>]+>\")\n",
    "\n",
    "sentence_regex = re.compile(\"[.!?]\")\n",
    "def split_to_sentences(text):\n",
    "    if len(set(sep_regex.findall(text)) | {\"<NEWLINE>\", \"<NEWPARAGRAPH>\"}) > 2:\n",
    "        return\n",
    "    for text_block in sep_regex.split(text):\n",
    "        for sentences in sentence_regex.split(text_block):\n",
    "            for sentence in sentences.split(\"\\n\"):\n",
    "                words = [w for w in word_regex.split(sentence) if len(w) > 0]\n",
    "                if len(words) < 1:\n",
    "                    continue\n",
    "                if len(words) > 20:\n",
    "                    continue\n",
    "                has_short_words = np.any([len(w1) <= 2 and len(w2) <= 2 for w1, w2 in zip(words, words[1:])])\n",
    "                if has_short_words:\n",
    "                    continue\n",
    "                yield words\n",
    "additional_corpus = additional_corpus.loc[~additional_corpus.text.isnull()]\n",
    "\n",
    "# set([x for xs in additional_corpus.text.apply(lambda x: sep_regex.findall(x)).values for x in xs])\n",
    "additional_corpus['cleaned_sentences'] = additional_corpus.text.apply(lambda x: list(split_to_sentences(x)))\n",
    "#additional_corpus.text.isnull().mean()\n",
    "flatten_sentences = [sentence for sentences in additional_corpus.cleaned_sentences.values for sentence in sentences if len(sentence) > 0]\n",
    "\n",
    "fixer.fill(flatten_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional_corpus.text.apply(lambda x: set(sep_regex.findall(x)) )\n",
    "len(fixer.vocabs[1].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ΜΕΤΑ', 'ΓΟΥΝ', 'ΑΛΛΑ', 'ΤΑ', 'ΠΟΛΛΑ', 'ΤΩΝ', 'ΕΡΩΤΟΧΑΡΙΤΩΝ'],\n",
       " ['ΟΣΑ', 'ΜΑΝΘΑΝΕΙ', 'ΦΥΣΙΚΩΣ', 'ΕΡΩΤΙΚΗ', 'ΚΑΡΔΙΑ'],\n",
       " ['ΕΣΕΒΗΣΑΝ', 'ΕΙΣ', 'ΤΟ', 'ΛΟΥΤΡΟΝ', 'ΕΛΟΥΣΘΗΣΑΝ', 'ΕΚΕΙΝΟΙ']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35711f7c1ae048f0b71554914f13d29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 3\n",
      "mt orig της δόξης του ιματισμου εφαιρειται μεντοι\n",
      "['της δόξης του ιματισμου εφαιρει ται μεντοι', 'της δόξης του ιματισμου ε φαιρειται μεντοι', 'της δόξης του ιματισμου εφαιρειται μεντοι']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ''}\n",
      "407 3\n",
      "mt orig εωρὸς ὸν οὐκδίασον ὐ καίνηρ χριρ\n",
      "['εωρὸς ὸν οὐ κ δίασον ὐ καίνηρ χριρ', 'εωρὸς ὸν οὐκ δίασ ο ν ὐ καίνηρ χριρ', 'εωρὸς ὸν οὐ κ δία σον ὐ καίνηρ χριρ']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ''}\n",
      "703 4\n",
      "mt orig δων δὲοις ότι επισυντρες\n",
      "['δων δὲ οις ότι επι συντρες', 'δων δὲ ο ις ότι επι συντρες', 'δων δὲ οις ότι επισυν τρες', 'δων δὲ ο ις ότι επισυν τρες']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ''}\n",
      "870 4\n",
      "mt orig ἀλλδκ εταυτνησιν ἀλλήδε κα ὼν\n",
      "['ἀλλδκ ετ αυτνησιν ἀλλή δε κα ὼν', 'ἀλλδκ ετα υ τνησιν ἀλλή δε κα ὼν', 'ἀλλδκ ε ταυτν ησιν ἀλλή δε κα ὼν', 'ἀλλδκ ε ταυτ νησιν ἀλλή δε κα ὼν']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ''}\n",
      "1034 3\n",
      "mt orig μηρλόντον νεαν ν δυσιμενοις τὸν\n",
      "['μηρλόντον νεαν ν δυσιμενοις τὸν', 'μηρλόντον νεαν ν δυσιμ ενοις τὸν', 'μηρλόντον νεαν ν δυσι μενοις τὸν']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ''}\n",
      "1135 3\n",
      "mt orig τις αρετῆς αναγίοιώτον διάκμειναι\n",
      "['τις αρετῆς αναγίοιώ τον διάκμειναι', 'τις αρετῆς αναγίοιώ τον διάκμ ειναι', 'τις αρετῆς αναγίοιώ τον διά κμ ειναι']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ''}\n",
      "1834 4\n",
      "mt orig α παυλος λεγων όθελων παντας ανουειστω\n",
      "['α παυλος λεγων ό θελων παντας ανουει σ τω', 'α παυλος λεγων ό θελων παντας ανου ειστω', 'α παυλος λεγων ό θελων παντας αν ουε ιστω', 'α παυλος λεγων ό θελων παντας α νου εισ τω']\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ''}\n"
     ]
    }
   ],
   "source": [
    "from common import correct_sigmas_in_word\n",
    "from common import fix_accent_diphthong\n",
    "\n",
    "def join_if_tuple(s):\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "    return \" \".join([(x) for x in s])\n",
    "\n",
    "def postprocess_spaces(spaces_after):\n",
    "    #return spaces_after\n",
    "    if len(spaces_after) < 2:\n",
    "        return spaces_after\n",
    "    if spaces_after[-1]!=\".\":\n",
    "        spaces_after[-1] = ''\n",
    "    return spaces_after\n",
    "\n",
    "\n",
    "corrected_texts = []\n",
    "# mt_sequences_train = mt_sequences_train[-1:]\n",
    "# mt_spaces_train = mt_spaces_train[-1:]\n",
    "# mt_texts_train = mt_texts_train[-1:]\n",
    "# ht_sequences_train = ht_sequences_train[-1:]\n",
    "corrected_count = 0\n",
    "corrected_raw = []\n",
    "train_iter = zip(\n",
    "    mt_sequences_train, \n",
    "    mt_spaces_train, \n",
    "    mt_texts_train, \n",
    "    ht_sequences_train)\n",
    "for i, (mt_words, line_spaces, mt_orig, ht_orig) in enumerate(tqdm(train_iter, total=len(mt_sequences_train))):\n",
    "    # print(line_spaces)\n",
    "    # print(mt_words)\n",
    "    # mt_orig = \" \".join(mt_words)\n",
    "    best = mt_orig\n",
    "    # temporary disable the following lines\n",
    "    replacements_ = [\n",
    "        [(w, s) for w, s in zip(mt_split, postprocess_spaces(spaces_after))]\n",
    "        for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=CUTOFF)\n",
    "    ]\n",
    "    replacements = [\n",
    "        [join_if_tuple(w) + s for w, s in replacement]\n",
    "        for replacement in replacements_\n",
    "    ]\n",
    "    replacements = [\"\".join(words) for words in replacements]\n",
    "\n",
    "    #replacements = [\n",
    "    #    (\"\".join([w + s for w, s in zip(mt_split, spaces_after)]), spaces_after) \n",
    "    #    \n",
    "    #]\n",
    "    # replacements, spaces_after = list(zip(*replacements))\n",
    "    # replacements = list(replacements)\n",
    "    if len(replacements) > 2:\n",
    "        print(i, len(replacements))\n",
    "        print(\"mt orig\", mt_orig)\n",
    "        print(replacements)\n",
    "        # print(spaces_after)\n",
    "        print(line_spaces)\n",
    "        # break\n",
    "    \n",
    "    best, best_index = lm_score(mt_orig, replacements,\n",
    "        lm=language_model, return_index=True\n",
    "        )\n",
    "    if best_index > 0:\n",
    "        corrected_count += 1\n",
    "        corrected_raw.append(replacements_[best_index - 1])\n",
    "    else:\n",
    "        corrected_raw.append(mt_orig)\n",
    "    corrected_texts.append(best)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df8d225bff640eeafecec3fe2dcf1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2363169651.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">34</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/2363169651.py'</span>          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/lacemaker/github/htrec-2022/notebooks/../src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">greedy.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">110</span> in                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">greedy_correction_one</span>                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 │   │   │   # print(\"fragment\", fragment)</span>                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   │   </span>new_fragments += fragment                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>110 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>fixed_fragment = fix_spaces(fragment, fixer, max_words=max_words)       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>new_fragments+= fixed_fragment                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   # fragments = [join_if_tuple(w) + s for w, s in new_fragments]</span>                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   # text = \"\".join(fragments)</span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/lacemaker/github/htrec-2022/notebooks/../src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">greedy.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">68</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fix_spaces</span>            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 65 │   │   │   </span>mt_word = Word([w <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> w, s <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> subseq])                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 66 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(mt_word) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 67 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 68 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ref_word = find_in_fixer(mt_word, fixer)                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> ref_word <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 │   │   │   </span>alignment_result = mt_word.distance_to(ref_word)                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/lacemaker/github/htrec-2022/notebooks/../src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">greedy.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_in_fixer</span>         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 39 │   │   </span>mt_word = Word(mt_word)                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 │   </span>N = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(fixer.vocabs)                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(N):                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 42 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> ref_word, tree_dist <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> fixer.vocabs[i + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>].find_closest(mt_word.sequenc <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> tree_dist != <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 44 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 45 │   │   │   </span>alignment_result = mt_word.distance_to(ref_word)                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/lacemaker/github/htrec-2022/notebooks/../src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">datastruct.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">314</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_closest</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">311 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">312 │   │   │   </span>word = Word(query_str)                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 │   │   </span>query_vector = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bow2vector(word.bow)                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>314 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>distances, indices = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.search_tree.query(query_vector.reshape(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>))    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 │   │   # print(indices, distances)</span>                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 │   │   </span>rad = np.min(distances)                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> distance_only:                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/\u001b[0m\u001b[1;33m2363169651.py\u001b[0m:\u001b[94m34\u001b[0m in      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m'/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/2363169651.py'\u001b[0m          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/Users/lacemaker/github/htrec-2022/notebooks/../src/\u001b[0m\u001b[1;33mgreedy.py\u001b[0m:\u001b[94m110\u001b[0m in                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92mgreedy_correction_one\u001b[0m                                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# print(\"fragment\", fragment)\u001b[0m                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_fragments += fragment                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m110 \u001b[2m│   │   │   \u001b[0mfixed_fragment = fix_spaces(fragment, fixer, max_words=max_words)       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_fragments+= fixed_fragment                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fragments = [join_if_tuple(w) + s for w, s in new_fragments]\u001b[0m                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# text = \"\".join(fragments)\u001b[0m                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/Users/lacemaker/github/htrec-2022/notebooks/../src/\u001b[0m\u001b[1;33mgreedy.py\u001b[0m:\u001b[94m68\u001b[0m in \u001b[92mfix_spaces\u001b[0m            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 65 \u001b[0m\u001b[2m│   │   │   \u001b[0mmt_word = Word([w \u001b[94mfor\u001b[0m w, s \u001b[95min\u001b[0m subseq])                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(mt_word) == \u001b[94m0\u001b[0m:                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 67 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 68 \u001b[2m│   │   │   \u001b[0mref_word = find_in_fixer(mt_word, fixer)                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m ref_word \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   │   │   \u001b[0malignment_result = mt_word.distance_to(ref_word)                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/Users/lacemaker/github/htrec-2022/notebooks/../src/\u001b[0m\u001b[1;33mgreedy.py\u001b[0m:\u001b[94m42\u001b[0m in \u001b[92mfind_in_fixer\u001b[0m         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 39 \u001b[0m\u001b[2m│   │   \u001b[0mmt_word = Word(mt_word)                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 40 \u001b[0m\u001b[2m│   \u001b[0mN = \u001b[96mmax\u001b[0m(fixer.vocabs)                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 41 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(N):                                                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 42 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m ref_word, tree_dist \u001b[95min\u001b[0m fixer.vocabs[i + \u001b[94m1\u001b[0m].find_closest(mt_word.sequenc \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 43 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m tree_dist != \u001b[94m0\u001b[0m:                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 44 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 45 \u001b[0m\u001b[2m│   │   │   \u001b[0malignment_result = mt_word.distance_to(ref_word)                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/Users/lacemaker/github/htrec-2022/notebooks/../src/\u001b[0m\u001b[1;33mdatastruct.py\u001b[0m:\u001b[94m314\u001b[0m in \u001b[92mfind_closest\u001b[0m     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m311 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   │   \u001b[0mword = Word(query_str)                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   \u001b[0mquery_vector = \u001b[96mself\u001b[0m.bow2vector(word.bow)                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m314 \u001b[2m│   │   \u001b[0mdistances, indices = \u001b[96mself\u001b[0m.search_tree.query(query_vector.reshape(\u001b[94m1\u001b[0m, -\u001b[94m1\u001b[0m))    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# print(indices, distances)\u001b[0m                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   \u001b[0mrad = np.min(distances)                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m distance_only:                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_tuple(s):\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "    if isinstance(s, tuple) and len(s) == 1:\n",
    "        return s[0]\n",
    "    return s\n",
    "def split_to_texts(seq):\n",
    "    prefix = []\n",
    "    for w, s in seq:\n",
    "        if isinstance(w, tuple):\n",
    "            if len(prefix) > 0:\n",
    "                prefix = [w+s for w, s in prefix]\n",
    "                yield \"\".join(prefix)\n",
    "                prefix = []\n",
    "            yield [(w, s)]\n",
    "        else:\n",
    "            prefix.append((w, s))\n",
    "    if len(prefix) > 0:\n",
    "        prefix = [w+s for w, s in prefix]\n",
    "        yield \"\".join(prefix)\n",
    "\n",
    "from greedy import greedy_correction, greedy_correction_one\n",
    "greedy_corrected_texts = []\n",
    "for text_seq in tqdm(corrected_raw):\n",
    "    if isinstance(text_seq, str):\n",
    "        text = greedy_correction_one(text_seq, fixer)\n",
    "        greedy_corrected_texts.append(text)\n",
    "    else:\n",
    "        text_seq = [(clean_tuple(w), s) for w, s in text_seq]\n",
    "        # break\n",
    "        all_fragments = []\n",
    "        for fragment in split_to_texts(text_seq):\n",
    "            if isinstance(fragment, str):\n",
    "                new_fragments = greedy_correction_one(fragment, fixer)\n",
    "                all_fragments += new_fragments\n",
    "            else:\n",
    "                all_fragments += fragment\n",
    "        greedy_corrected_texts.append(all_fragments)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_corrected_texts = [\n",
    "    \"\".join([join_if_tuple(w) + s for w, s in text])\n",
    "    for text in greedy_corrected_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=5):\n",
    "# #     print(mt_split, refs, spaces_after)\n",
    "# # # print(ht_orig)\n",
    "# # replacements\n",
    "# # greedy_correction(mt_sequences_train, fixer)\n",
    "# from greedy import greedy_correction\n",
    "# splitted_texts = [word_regex2.split(text) for text in corrected_texts]\n",
    "# corrected_texts = []\n",
    "# corrected_count = 0\n",
    "# for corrected_seq in greedy_correction(splitted_texts, fixer):\n",
    "#     text = [join_if_tuple(w) + s for w, s in corrected_seq]\n",
    "#     text = \"\".join(text)\n",
    "#     corrected_texts.append(text)\n",
    "#     if text != mt_orig:\n",
    "#         corrected_count += 1\n",
    "# corrected_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n",
      "487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει',\n",
       " 'του β ου του καλ εαυτοὺς πολλα γινεσθαι συγχωρ όν',\n",
       " 'τες εμπυριζου σιμαμπελῶνα ἀλλα και ὅδξα']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.name(\"σ\"), unicodedata.name(\"ς\")\n",
    "print(corrected_count)\n",
    "# fixer.split_matrix\n",
    "# best, is_corrected = lm_score(mt_orig, replacements, lm=language_model, return_corrected=True)\n",
    "# best, ht_orig\n",
    "count = np.sum([mt_orig!= corrected for mt_orig, corrected in zip(mt_texts_train, greedy_corrected_texts)])\n",
    "print(count)\n",
    "greedy_corrected_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 6\n",
    "# train_iter = zip(\n",
    "#     mt_sequences_train[K:],\n",
    "#     mt_spaces_train[K:],\n",
    "#     mt_texts_train[K:],\n",
    "#     ht_sequences_train[K:]\n",
    "# )\n",
    "\n",
    "# for i, (mt_words, line_spaces, mt_orig, ht_orig) in enumerate(tqdm(train_iter, total=len(mt_sequences_train[K:]))):\n",
    "#     break\n",
    "# print(\"mt_original: \", mt_words)\n",
    "# print(\"ht:\", ht_orig)\n",
    "# for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=0):\n",
    "#     print(mt_split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified:\n",
      "Candidate CER: 33.326478596708895\n",
      "Candidate CERR: 0.9285963805836962\n",
      "Corrected sigmas:\n",
      "Candidate CER: 37.41469122427831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3258382041.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> in      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/3258382041.py'</span>          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/lacemaker/github/htrec-2022/notebooks/../src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metrics.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_metrics</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Candidate CER: {</span>cer_values.mean()<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11   # Computing the character error *reduction* rate (CERR)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12   </span>cer_ht_st = compute_cer_values(human_translations, system_translations)            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>13 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Candidate CERR: {</span>(cer_ht_st - cer_values).mean()<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cer_ht_st, cer_values                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>operands could not be broadcast together with shapes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1875</span>,<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1532</span>,<span style=\"font-weight: bold\">)</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/\u001b[0m\u001b[1;33m3258382041.py\u001b[0m:\u001b[94m11\u001b[0m in      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[3;31m'/var/folders/h6/ssrt88hx24gg57sp1hgv1ftm0000gn/T/ipykernel_95437/3258382041.py'\u001b[0m          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/Users/lacemaker/github/htrec-2022/notebooks/../src/\u001b[0m\u001b[1;33mmetrics.py\u001b[0m:\u001b[94m13\u001b[0m in \u001b[92mcompute_metrics\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m  \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCandidate CER: \u001b[0m\u001b[33m{\u001b[0mcer_values.mean()\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m  \u001b[0m\u001b[2m# Computing the character error *reduction* rate (CERR)\u001b[0m                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m  \u001b[0mcer_ht_st = compute_cer_values(human_translations, system_translations)            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m13 \u001b[2m  \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCandidate CERR: \u001b[0m\u001b[33m{\u001b[0m(cer_ht_st - cer_values).mean()\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mreturn\u001b[0m cer_ht_st, cer_values                                                       \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0moperands could not be broadcast together with shapes \u001b[1m(\u001b[0m\u001b[1;36m1875\u001b[0m,\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m1532\u001b[0m,\u001b[1m)\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from common import fix_accents\n",
    "ht_texts = train_df.HUMAN_TRANSCRIPTION.values\n",
    "mt_texts = train_df.SYSTEM_TRANSCRIPTION_raw.values\n",
    "ct = [lmr(t, lm=language_model) for t in greedy_corrected_texts]\n",
    "ct2 = [fix_accent_diphthong(t) for t in ct]\n",
    "ct3 = [fix_accents(t) for t in ct2]\n",
    "# ct2 = [lmr(t, word=\" δ \", replacements=[\"δ \", \" δ\", \" \"], lm=language_model) for t in ct]\n",
    "print(\"Unmodified:\")\n",
    "cerr_values_ht, cerr_values = compute_metrics(ht_texts, mt_texts, corrected_texts)\n",
    "print(\"Corrected sigmas:\")\n",
    "cerr_values_ht1, cerr_values1 = compute_metrics(ht_texts, mt_texts, ct)\n",
    "print(\"corrected deltas:\")\n",
    "cerr_values_ht2, cerr_values2 = compute_metrics(ht_texts, mt_texts, ct2)\n",
    "print(\"accents:\")\n",
    "cerr_values_ht2, cerr_values2 = compute_metrics(ht_texts, mt_texts, ct3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.57142857142857\n",
      "'τις αρετῆς αναγίοιώ τον διά κμ ειναι'\n",
      "'τῆς ἀρετῆς ἀναλλοίωτον διαμεῖναι. Ἡ' 'τις αρετῆς αναγίοιώτον διάκμειναι'\n",
      "-5.555555555555557\n",
      "'εἐργων τῆς δι και ουνος ὶ ειπης ψγ'\n",
      "'ἔργων τῆς δικαιοσύνης. Μή εἴπῃς, γάρ' 'εἐργων τῆς δικαιουνος ὶ ειπης ψγ'\n",
      "-5.555555555555557\n",
      "'σιν μποιοξαν κὐ ταχὴν προ τάρ ηες αν'\n",
      "'σοὶ ξυμπονῆσαι καὶ ταχὺν προσαρκέσαι' 'σιν μποιοξαν κὐ ταχὴν προτάρηες αν'\n",
      "-5.555555555555557\n",
      "'τοῦ δυσιν εσαιλόγος σξωην ἐκρ'\n",
      "'τοῦ Θῦ γίνεται Λόγος, ὡς ζωή νεκροῖς' 'τοῦ δυσιν εσαιλόγος ς ξωην ἐκρ'\n",
      "-5.263157894736835\n",
      "'ὁ γι ότε α τις α τειλεισαρνλκαι εφυλαζεητόν'\n",
      "'Ὁπότε ἀπέστειλε Σαοὺλ, καὶ ἐφύλαξε τὸν' 'ὁ γιότεατις α τειλεισαρνλκαι εφυλαζεητόν'\n"
     ]
    }
   ],
   "source": [
    "diff_cer = (cerr_values_ht - cerr_values)\n",
    "idx = np.argsort(diff_cer)\n",
    "for i in idx[:5]:\n",
    "    print(diff_cer[i])\n",
    "    print(repr(ct[i]))\n",
    "    print(repr(ht_texts[i]), repr(mt_texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"SYSTEM_TRANSCRIPTION_raw\"] = test_df.SYSTEM_TRANSCRIPTION\n",
    "test_df[\"SYSTEM_TRANSCRIPTION\"] = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: lmr(x, lm=language_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d682a98e2c84d19bf7d6dfb5508e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_orig_test = test_df.SYSTEM_TRANSCRIPTION.values\n",
    "mt_sequences_test = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_spaces_test = test_df.SYSTEM_TRANSCRIPTION.apply(extract_spaces_dict).values\n",
    "corrected_texts_test = []\n",
    "corrected_raw_test = []\n",
    "corrected_count_test = 0\n",
    "for i, (mt_words, line_spaces, mt_orig) in enumerate(tqdm(zip(mt_sequences_test, mt_spaces_test, mt_orig_test), total=len(mt_sequences_test))):\n",
    "    # print(i)\n",
    "    # print(mt_words)\n",
    "    # mt_orig = \" \".join(mt_words)\n",
    "    # temporary disable the following lines\n",
    "    best = mt_orig\n",
    "    # replacements = [\"\".join([w+s for w, s in zip(mt_split, spaces_after)]) for mt_split, refs, spaces_after in fixer.split_words(mt_words, mt_spaces)]\n",
    "    replacements_ = [\n",
    "        [(w, s) for w, s in zip(mt_split, postprocess_spaces(spaces_after))]\n",
    "        for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=CUTOFF)\n",
    "    ]\n",
    "    replacements = [\n",
    "        [join_if_tuple(w) + s for w, s in replacement]\n",
    "        for replacement in replacements_\n",
    "    ]\n",
    "    replacements = [\"\".join(words) for words in replacements]\n",
    "    best, best_index = lm_score(\n",
    "        mt_orig, replacements,\n",
    "        lm=language_model, return_index=True\n",
    "    )\n",
    "    if best_index > 0:\n",
    "        corrected_count_test += 1\n",
    "        corrected_raw_test.append(replacements_[best_index - 1])\n",
    "    else:\n",
    "        corrected_raw_test.append(mt_orig)\n",
    "\n",
    "    # best = lm_score(mt_orig, replacements, lm=language_model)\n",
    "    corrected_texts_test.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9063e98d3b864859b1d7c29d97a3adcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "greedy_corrected_texts_test = []\n",
    "for text_seq in tqdm(corrected_raw_test):\n",
    "    if isinstance(text_seq, str):\n",
    "        text = greedy_correction_one(text_seq, fixer)\n",
    "        greedy_corrected_texts_test.append(text)\n",
    "    else:\n",
    "        text_seq = [(clean_tuple(w), s) for w, s in text_seq]\n",
    "        # break\n",
    "        all_fragments = []\n",
    "        for fragment in split_to_texts(text_seq):\n",
    "            if isinstance(fragment, str):\n",
    "                new_fragments = greedy_correction_one(fragment, fixer)\n",
    "                all_fragments += new_fragments\n",
    "            else:\n",
    "                all_fragments += fragment\n",
    "        greedy_corrected_texts_test.append(all_fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy_corrected_texts_test\n",
    "greedy_corrected_texts_test = [\n",
    "    \"\".join([join_if_tuple(w) + s for w, s in text])\n",
    "    for text in greedy_corrected_texts_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "7\n",
      "60\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "corrected_texts_test1 = [lmr(t, lm=language_model) for t in greedy_corrected_texts_test]\n",
    "print(np.sum([t1!= t for t1, t in zip(corrected_texts_test1, corrected_texts_test)]))\n",
    "# corrected_texts_test2 = [\n",
    "#     lmr(t, word=\" δ \", replacements=[\"δ \", \" δ\", \" \"], lm=language_model)\n",
    "#     for t in corrected_texts_test1\n",
    "# ]\n",
    "corrected_texts_test2 = [fix_accent_diphthong(t) for t in corrected_texts_test1]\n",
    "print(np.sum([t1 != t for t1, t in zip(corrected_texts_test2, corrected_texts_test)]))\n",
    "print(np.sum([t1 != t for t1, t in zip(corrected_texts_test2, mt_orig_test)]))\n",
    "corrected_texts_test3 = [fix_accents(t) for t in corrected_texts_test2]\n",
    "print(np.sum([t1 != t for t1, t in zip(corrected_texts_test3, corrected_texts_test2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ὑπρ τη συνευςδιξοεανδνυπορ ποοδυπρας',\n",
       " 'ἢ μητακχεφιλοχορ εύτα δυ μπρο πρενπεμε',\n",
       " 'συ σ κατεχισωμε ἐπι τελωτι',\n",
       " 'κ ἀπαὐτελετία τὸν δεὲ τὸν σανἀαλισκόν']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_texts_test2[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Transcriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>105 Bodleian-Library-MS-Barocci-59_00085_fol-4...</td>\n",
       "      <td>τὲ πρμον ην τὴν αν θησιν τὴν σρ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ImageID  \\\n",
       "110  105 Bodleian-Library-MS-Barocci-59_00085_fol-4...   \n",
       "\n",
       "                      Transcriptions  \n",
       "110  τὲ πρμον ην τὴν αν θησιν τὴν σρ  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(\n",
    "    zip(test_df.IMAGE_PATH, corrected_texts_test3),\n",
    "    columns=[\"ImageID\", \"Transcriptions\"]\n",
    ")\n",
    "submission.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">submission.csv</span> <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100.0%</span> • <span style=\"color: #008000; text-decoration-color: #008000\">45.4/43.7 KB</span> • <span style=\"color: #800000; text-decoration-color: #800000\">513.9 kB/s</span> • <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34msubmission.csv\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100.0%\u001b[0m • \u001b[32m45.4/43.7 KB\u001b[0m • \u001b[31m513.9 kB/s\u001b[0m • \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                    ╭─────────────────────────╮                                     \n",
       "                                    │ <span style=\"font-weight: bold\">Successfully submitted!</span> │                                     \n",
       "                                    ╰─────────────────────────╯                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                    ╭─────────────────────────╮                                     \n",
       "                                    │ \u001b[1mSuccessfully submitted!\u001b[0m │                                     \n",
       "                                    ╰─────────────────────────╯                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Important links                                           </span>\n",
       "┌──────────────────┬───────────────────────────────────────────────────────────────────────────────┐\n",
       "│  This submission │ https://www.aicrowd.com/challenges/htrec-2022/submissions/191928              │\n",
       "│                  │                                                                               │\n",
       "│  All submissions │ https://www.aicrowd.com/challenges/htrec-2022/submissions?my_submissions=true │\n",
       "│                  │                                                                               │\n",
       "│      Leaderboard │ https://www.aicrowd.com/challenges/htrec-2022/leaderboards                    │\n",
       "│                  │                                                                               │\n",
       "│ Discussion forum │ https://discourse.aicrowd.com/c/htrec-2022                                    │\n",
       "│                  │                                                                               │\n",
       "│   Challenge page │ https://www.aicrowd.com/challenges/htrec-2022                                 │\n",
       "└──────────────────┴───────────────────────────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                          Important links                                           \u001b[0m\n",
       "┌──────────────────┬───────────────────────────────────────────────────────────────────────────────┐\n",
       "│  This submission │ https://www.aicrowd.com/challenges/htrec-2022/submissions/191928              │\n",
       "│                  │                                                                               │\n",
       "│  All submissions │ https://www.aicrowd.com/challenges/htrec-2022/submissions?my_submissions=true │\n",
       "│                  │                                                                               │\n",
       "│      Leaderboard │ https://www.aicrowd.com/challenges/htrec-2022/leaderboards                    │\n",
       "│                  │                                                                               │\n",
       "│ Discussion forum │ https://discourse.aicrowd.com/c/htrec-2022                                    │\n",
       "│                  │                                                                               │\n",
       "│   Challenge page │ https://www.aicrowd.com/challenges/htrec-2022                                 │\n",
       "└──────────────────┴───────────────────────────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'submission_id': 191928, 'created_at': '2022-07-02T02:38:31.675Z'}\n"
     ]
    }
   ],
   "source": [
    "%aicrowd submission create -c htrec-2022 -f submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_spaces = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values\n",
    "#train_spaces1 = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values\n",
    "#train_spaces2 = train_df.HUMAN_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#Counter([x for xs in test_spaces for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter([x for xs in train_spaces1 for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([x for xs in train_spaces2 for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_corrected = 0\n",
    "# for corrected, mt_orig, ht_orig in zip(\n",
    "#         corrected_texts, mt_texts_train, ht_sequences_train):\n",
    "#     if mt_orig==corrected:\n",
    "#         print(repr(mt_orig))\n",
    "#         print( repr(corrected))\n",
    "#         print(ht_orig)\n",
    "#         print(\"---\")\n",
    "#         n_corrected+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def has_accents(text):\n",
    "#     # print(text)\n",
    "#     accents = [unicodedata.name(x).find(\" WITH \") > 0 for word in text for x in word]\n",
    "#     return np.any(accents)\n",
    "# def extract_short_words(text, n=3):\n",
    "#     # print(text)\n",
    "#     return [word for word in text if len(word) == n]\n",
    "# info = []\n",
    "# for mt_text, ht_orig in zip(mt_texts_train, ht_sequences_train):\n",
    "#     mt_orig = word_regex.split(mt_text)\n",
    "#     info.append({\n",
    "#         \"mt_accent\": has_accents(mt_orig),\n",
    "#         \"ht_accent\": has_accents(ht_orig),\n",
    "#         \"mt_short3\": extract_short_words(mt_orig, 3),\n",
    "#         \"ht_short3\": extract_short_words(ht_orig, 3)\n",
    "#     })\n",
    "#     # print(info)\n",
    "#     #break\n",
    "# df = pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[\"mt_accent\", \"ht_accent\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['accent'] = df.ht_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.groupby(\"CENTURY\").mean()[\"accent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from common import remove_caps\n",
    "# df.groupby(\"ht_accent\").mean()[\"mt_accent\"]\n",
    "# ht_words = [x for xs in df[df[\"ht_accent\"]].ht_short3.values for x in xs \n",
    "#  if remove_cap(x) == remove_cap(\"μου\")\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# sorted(Counter(ht_words).items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([x for xs in df.mt_short3.values for x in xs if remove_cap(x) == remove_cap(\"μου\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d632e93423067b2b9b1b8d52846f85c868da433699adad534ad2cc6673775271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
