{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seeds, make everything reproducible, etc (at least try to).\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "DATADIR = \"../data\"\n",
    "\n",
    "MAX_WORDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aicrowd.magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext aicrowd.magic\n"
     ]
    }
   ],
   "source": [
    "# !pip install aicrowd-cli\n",
    "%load_ext aicrowd.magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAPI Key valid\u001b[0m\n",
      "\u001b[33mGitlab oauth token invalid or absent.\n",
      "It is highly recommended to simply run `aicrowd login` without passing the API Key.\u001b[0m\n",
      "\u001b[32mSaved details successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%aicrowd login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/explosion/spaCy/blob/657af5f91f88bb5a414ae133a99465dbc4f240be/spacy/lang/grc/stop_words.py\n",
    "# we can use stopwords from spacy as a source of \"known\" short words.\n",
    "# but the rules say that we should use only data from the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f301a15b8a40389390d0a3e2c44c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/395k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aeed257acb4f02b4a6c9f0220503c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/45.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "if Path(DATADIR).exists():\n",
    "  !rm -rf $DATADIR\n",
    "!mkdir $DATADIR\n",
    "%aicrowd ds dl -c htrec-2022 -o $DATADIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUMAN_TRANSCRIPTION</th>\n",
       "      <th>SYSTEM_TRANSCRIPTION</th>\n",
       "      <th>CENTURY</th>\n",
       "      <th>IMAGE_PATH</th>\n",
       "      <th>TEXT_LINE_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει</td>\n",
       "      <td>ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...</td>\n",
       "      <td>του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ</td>\n",
       "      <td>τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>τῆς ἡδεῖας πλεονεξίας πολλοὺς εἰς τὴν τῶν ἀλλ</td>\n",
       "      <td>της ἐδίας πλσον ἐξιας πολλους ἐις τὴν τῶν ἀλ</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>οτρίων ἐπιθυμίαν προκαλούμενος ἐμπυρί</td>\n",
       "      <td>λοτρλων ἐπιθυμιαν προκαλουμένος ἐμπυρι</td>\n",
       "      <td>11</td>\n",
       "      <td>1 Bodleian-Library-MS-Barocci-102_00157_fol-75...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 HUMAN_TRANSCRIPTION  \\\n",
       "0      ἐγγινομένα πάθη μὴ σβεννύντες ἀλλὰ τῆ εκλύσει   \n",
       "1  τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχ...   \n",
       "2        τες ἐμπυρίζουσι τὸν ἀμπελῶνα ἀλλὰ καὶ ὁ διὰ   \n",
       "3      τῆς ἡδεῖας πλεονεξίας πολλοὺς εἰς τὴν τῶν ἀλλ   \n",
       "4              οτρίων ἐπιθυμίαν προκαλούμενος ἐμπυρί   \n",
       "\n",
       "                              SYSTEM_TRANSCRIPTION  CENTURY  \\\n",
       "0         ἐγγενομεναπαδημησμεννωτες ἀλλατῆε κλησει       11   \n",
       "1  του β ου του καλεαυτοὺς πολλαγινεσθαι συγχωρ όν       11   \n",
       "2           τες εμπυριζου σιμαμπελῶνα ἀλλακαι ὅδξα       11   \n",
       "3     της ἐδίας πλσον ἐξιας πολλους ἐις τὴν τῶν ἀλ       11   \n",
       "4           λοτρλων ἐπιθυμιαν προκαλουμένος ἐμπυρι       11   \n",
       "\n",
       "                                          IMAGE_PATH  TEXT_LINE_NUM  \n",
       "0  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              1  \n",
       "1  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              2  \n",
       "2  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              3  \n",
       "3  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              4  \n",
       "4  1 Bodleian-Library-MS-Barocci-102_00157_fol-75...              5  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pywer\n",
    "train_df = pd.read_csv( f\"{DATADIR}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{DATADIR}/test.csv\")\n",
    "\n",
    "word_regex = re.compile(\"\\W+\")\n",
    "word_regex2 = re.compile(\"(\\W+)\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# train_df.HUMAN_TRANSCRIPTION.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lib_dir in [\"..\", \"../src\"]:\n",
    "    if not lib_dir in sys.path:\n",
    "        sys.path.append(lib_dir)\n",
    "from lm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = make_lm(train_df.HUMAN_TRANSCRIPTION.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datastruct import *\n",
    "from common import *\n",
    "from space_fixer import SpaceFixer\n",
    "\n",
    "# for ht_line, mt_line in tqdm(train_df[[\"HUMAN_TRANSCRIPTION\", \"SYSTEM_TRANSCRIPTION\"]].values[7:]):\n",
    "#     ht_words = word_regex.split(ht_line)\n",
    "#     mt_words = word_regex.split(mt_line)\n",
    "#     #words_ = [remove_cap(word) for word in words]\n",
    "#     #vocab.add_sentence(words)\n",
    "#     dmatrix = build_path_matrix(mt_words, vocabs)\n",
    "#     finished_paths = extract_paths(dmatrix)\n",
    "#     for k in resplit_paths(finished_paths, mt_words):\n",
    "#         variant = \" \".join(k)\n",
    "#         print(variant)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spaces_dict(x):\n",
    "    result = dict()\n",
    "    last_index = 0\n",
    "    for w in word_regex2.split(x):\n",
    "        if not word_regex.match(w):\n",
    "            last_index += 1\n",
    "        else:\n",
    "            result[last_index] = w\n",
    "    if not last_index in result:\n",
    "        result[last_index] = \"\"\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b69c89b0e344726a1df36675aa34a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_df[\"SYSTEM_TRANSCRIPTION_raw\"] = train_df.SYSTEM_TRANSCRIPTION\n",
    "train_df[\"SYSTEM_TRANSCRIPTION\"] = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: lmr(x, lm=language_model))\n",
    "\n",
    "ht_sequences_train = train_df.HUMAN_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_sequences_train = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_spaces_train = train_df.SYSTEM_TRANSCRIPTION.apply(extract_spaces_dict).values\n",
    "\n",
    "mt_texts_train = train_df.SYSTEM_TRANSCRIPTION.values\n",
    "fixer = SpaceFixer(MAX_WORDS)\n",
    "fixer.fill(ht_sequences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [w for w in fixer.vocabs[1].get_words(1)]\n",
    "words2 = [w for w in fixer.vocabs[1].get_words(2)]\n",
    "stopwords = words1 + words2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10833ffae0564809b14cbc7502d25a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def join_if_tuple(s):\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "    return \" \".join(s)\n",
    "\n",
    "corrected_texts = []\n",
    "# mt_sequences_train = mt_sequences_train[-1:]\n",
    "# mt_spaces_train = mt_spaces_train[-1:]\n",
    "# mt_texts_train = mt_texts_train[-1:]\n",
    "# ht_sequences_train = ht_sequences_train[-1:]\n",
    "corrected_count = 0\n",
    "k = 0\n",
    "train_iter = zip(mt_sequences_train[k:], mt_spaces_train[k:], mt_texts_train[k:], ht_sequences_train[k:])\n",
    "for i, (mt_words, line_spaces, mt_orig, ht_orig) in enumerate(tqdm(train_iter, total=len(mt_sequences_train))):\n",
    "    # print(line_spaces)\n",
    "    # print(mt_words)\n",
    "    # mt_orig = \" \".join(mt_words)\n",
    "    best = mt_orig\n",
    "    # temporary disable the following lines\n",
    "    replacements = [\n",
    "        [join_if_tuple(w) + s for w, s in zip(mt_split, spaces_after)]\n",
    "        for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=3)\n",
    "    ]\n",
    "    replacements = [\"\".join(words) for words in replacements]\n",
    "\n",
    "    #replacements = [\n",
    "    #    (\"\".join([w + s for w, s in zip(mt_split, spaces_after)]), spaces_after) \n",
    "    #    \n",
    "    #]\n",
    "    # replacements, spaces_after = list(zip(*replacements))\n",
    "    # replacements = list(replacements)\n",
    "    if len(replacements) > 2:\n",
    "        print(i, len(replacements))\n",
    "        print(replacements)\n",
    "        # print(spaces_after)\n",
    "        print(line_spaces)\n",
    "        # break\n",
    "    \n",
    "    best, is_corrected = lm_score(mt_orig, replacements, lm=language_model, return_corrected=True)\n",
    "    if is_corrected:\n",
    "        corrected_count += 1\n",
    "        # break\n",
    "    corrected_texts.append(best)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt_words, line_spaces, best, ht_orig\n",
    "# class WordInfo:\n",
    "#     def __init__(self, word, is_known=False, is_space=False):\n",
    "#         self.word = word\n",
    "#         self.is_known = is_known\n",
    "#         self.is_space = is_space\n",
    "#     def __repr__(self):\n",
    "#         return f\"{self.word}, is known={self.is_known}, is space={self.is_space}\"\n",
    "# info = []\n",
    "# for w in mt_words:\n",
    "#     dist = fixer.vocabs[1].find_closest(w, distance_only=True)\n",
    "#     info.append(WordInfo(w, is_known = dist == 0))\n",
    "# info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.name(\"σ\"), unicodedata.name(\"ς\")\n",
    "corrected_count\n",
    "# fixer.split_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mt_split, refs, spaces_after in fixer.resplit(mt_words, line_spaces):\n",
    "#     print(mt_split, spaces_after, line_spaces)\n",
    "\n",
    "# ht_orig, mt_orig\n",
    "def postprocess_sigmas(sentence):\n",
    "    def correct_word(word):\n",
    "        if word_regex.match(word):\n",
    "            return word\n",
    "        #\"ς\"\n",
    "        cw = []\n",
    "        if len(word) < 1:\n",
    "            return word\n",
    "        for c in word[:-1]:\n",
    "            name = unicodedata.name(c)\n",
    "            new_char = c\n",
    "            if name.find(\"FINAL SIGMA\") >= 0:\n",
    "                name = name.replace(\"FINAL \", \"\")\n",
    "                try:\n",
    "                    new_char = unicodedata.lookup(name)\n",
    "                except:\n",
    "                    new_char = c\n",
    "            cw.append(new_char)\n",
    "        c = word[-1]\n",
    "        name = unicodedata.name(c)\n",
    "        if name.find(\"SIGMA\") >= 0 and name.find(\"FINAL SIGMA\") < 0:\n",
    "            name = name.replace(\"SIGMA\", \"FINAL SIGMA\")\n",
    "            try:\n",
    "                new_char = unicodedata.lookup(name)\n",
    "            except:\n",
    "                new_char = c\n",
    "        cw.append(c)\n",
    "        return \"\".join(cw)\n",
    "\n",
    "    words = [correct_word(x) for x in word_regex2.split(sentence)]\n",
    "    return \"\".join(words)\n",
    "    \n",
    "\n",
    "# [postprocess_sigmas(t) for t in corrected_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ς', 'ζ', 'λ', 'π', 'β']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(ht_sequences_train[-1])\n",
    "#print(\"mt:\", mt_words)\n",
    "#for k, v, spaces_after in fixer.split_words(mt_words, line_spaces):\n",
    "#    print(k, v, spaces_after)\n",
    "\n",
    "def get_short_words(texts, max_n=2):\n",
    "    return set([w for line in texts for w in word_regex.split(line) if len(w) <= max_n and len(w) > 0])\n",
    "\n",
    "VOWELS = set([\"α\", \"ε\", \"η\" \"ι\", \"ο\", \"υ\", \"ω\"])\n",
    "def is_vowel(ch):\n",
    "    return remove_cap(ch) in VOWELS\n",
    "    \n",
    "short_words = get_short_words(corrected_texts, max_n=1)\n",
    "\n",
    "unknown_words = []\n",
    "for w in short_words:\n",
    "    word = Word(w)\n",
    "    is_known = np.any([word.no_caps == w2.no_caps for w2 in words1])\n",
    "    if not is_known:\n",
    "        unknown_words.append(w)\n",
    "\n",
    "unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixer.dmatrix\n",
    "#fixer.split_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified:\n",
      "Candidate CER: 33.345936213460185\n",
      "Candidate CERR: 0.9091387638324059\n",
      "Corrected sigmas:\n",
      "Candidate CER: 33.34449477201874\n",
      "Candidate CERR: 0.9105802052738475\n"
     ]
    }
   ],
   "source": [
    "ht_texts = train_df.HUMAN_TRANSCRIPTION.values\n",
    "mt_texts = train_df.SYSTEM_TRANSCRIPTION_raw.values\n",
    "ct = [lmr(t, word=\" ς \", replacements=[\"ς \", \" \", \" σ \", \" σ\", \" ὡς \"], lm=language_model) for t in corrected_texts]\n",
    "# ct2 = [postprocess_sigmas(t) for t in ct]\n",
    "# ct2 = [lmr(t, word=\" δ \", replacements=[\"δ \", \" δ\", \" \"], lm=language_model) for t in ct]\n",
    "print(\"Unmodified:\")\n",
    "cerr_values_ht, cerr_values = compute_metrics(ht_texts, mt_texts, corrected_texts)\n",
    "print(\"Corrected sigmas:\")\n",
    "cerr_values_ht1, cerr_values1 = compute_metrics(ht_texts, mt_texts, ct)\n",
    "# print(\"corrected deltas:\")\n",
    "# cerr_values_ht2, cerr_values2 = compute_metrics(ht_texts, mt_texts, ct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class WordToken:\n",
    "    word: str\n",
    "    is_word: bool\n",
    "def resplit_text(text):\n",
    "    return [\n",
    "        WordToken(w, word_regex.match(w) is None) for w in word_regex2.split(text)\n",
    "    ]\n",
    "\n",
    "def make_variants(text):\n",
    "    tokens_list = resplit_text(text)\n",
    "    variants = []\n",
    "    prefixes = []\n",
    "    for i, wt in enumerate(tokens_list):\n",
    "        if not wt.is_word and len(prefixes) == 0:\n",
    "            prefixes = [[wt]]\n",
    "            continue\n",
    "        if not wt.is_word:\n",
    "            continue\n",
    "        new_prefixes = []\n",
    "        if remove_cap(wt.word) in unknown_words:\n",
    "            # operate\n",
    "            if len(prefixes) == 0:\n",
    "                if i < len(tokens_list) - 1:\n",
    "                    next_token = [tokens_list[i + 1]]\n",
    "                    new_prefixes.append([wt])\n",
    "                else:\n",
    "                    next_token = []\n",
    "                new_prefixes.append([wt] + next_token)\n",
    "                # new_prefixes.append(p)\n",
    "            else:\n",
    "                #next_token = tokens_list[i+1]\n",
    "                for p in prefixes:\n",
    "                    if i < len(tokens_list) - 1:\n",
    "                        next_token = [tokens_list[i + 1]]\n",
    "                        new_prefixes.append(p + [wt])\n",
    "                    else:\n",
    "                        next_token = []\n",
    "                    if len(p) > 0:\n",
    "                        new_prefixes.append(p[:-1] +[wt] + next_token)\n",
    "                    # else:\n",
    "                    #     new_prefixes.append([wt])\n",
    "                    new_prefixes.append(p + [wt] + next_token)\n",
    "                    new_prefixes.append(p)\n",
    "            \n",
    "        else:\n",
    "            if i < len(tokens_list) - 1:\n",
    "                next_token = [tokens_list[i + 1]]\n",
    "            else:\n",
    "                next_token = []\n",
    "            # don't modify\n",
    "            if len(prefixes) == 0:\n",
    "                new_prefixes.append([wt] + next_token)\n",
    "            else:\n",
    "                # print(prefixes)\n",
    "                new_prefixes.extend([prefix + [wt] + next_token for prefix in prefixes])\n",
    "        prefixes = new_prefixes\n",
    "    for prefix in prefixes:\n",
    "        yield \"\".join([word.word for word in prefix])\n",
    "\n",
    "corrected_texts2 = []\n",
    "for text in corrected_texts:\n",
    "    variants = list(make_variants(text))\n",
    "    best = lm_score(text, variants, lm=language_model)\n",
    "    corrected_texts2.append(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate CER: 33.34485527959311\n",
      "Candidate CERR: 0.9102196976994789\n"
     ]
    }
   ],
   "source": [
    "cerr_values_ht2, cerr_values2 = compute_metrics(ht_texts, mt_texts, corrected_texts2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_texts2\n",
    "# diff = cerr_values_ht - cerr_values\n",
    "# ids = np.argsort(diff)\n",
    "# for i in ids[:20]:\n",
    "#     if corrected_texts[i] == mt_texts[i]:\n",
    "#         continue\n",
    "#     print(i, diff[i])\n",
    "#     print(repr(corrected_texts[i]))\n",
    "#     print(\"ht:\", repr(ht_texts[i]))\n",
    "#     print(\"mt:\", repr(mt_texts[i]))\n",
    "#     print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(unknown_words)\n",
    "# ct_mod = ct\n",
    "# # unknown_words = \"β ζ λ π ς\".split()\n",
    "# for w in unknown_words:\n",
    "#     ct_mod = [\n",
    "#         lmr(t, word=f\" {w} \", replacements=[f\"{w} \", f\" {w}\", \" \"], lm=language_model)\n",
    "#         for t in ct_mod\n",
    "#     ]\n",
    "#     print(\"Word:\", w)\n",
    "#     cerr_values_ht3, cerr_values3 = compute_metrics(ht_texts, mt_texts, ct_mod)\n",
    "#     print(\"--\")\n",
    "# # unknown_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_cer = (cerr_values_ht3 - cerr_values3)\n",
    "# idx = np.argsort(diff_cer)\n",
    "# for i in idx[:5]:\n",
    "#     print(diff_cer[i])\n",
    "#     print(repr(ct2[i]))\n",
    "#     print(repr(ht_texts[i]), repr(mt_texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"SYSTEM_TRANSCRIPTION_raw\"] = test_df.SYSTEM_TRANSCRIPTION\n",
    "test_df[\"SYSTEM_TRANSCRIPTION\"] = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: lmr(x, lm=language_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756d226dd6cb405ea23c00e949ba3295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_orig_test = test_df.SYSTEM_TRANSCRIPTION.values\n",
    "mt_sequences_test = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: word_regex.split(x)).values\n",
    "mt_spaces_test = test_df.SYSTEM_TRANSCRIPTION.apply(extract_spaces_dict).values\n",
    "corrected_texts_test = []\n",
    "for i, (mt_words, line_spaces, mt_orig) in enumerate(tqdm(zip(mt_sequences_test, mt_spaces_test, mt_orig_test), total=len(mt_sequences_test))):\n",
    "    # print(i)\n",
    "    # print(mt_words)\n",
    "    # mt_orig = \" \".join(mt_words)\n",
    "    # temporary disable the following lines\n",
    "    best = mt_orig\n",
    "    # replacements = [\"\".join([w+s for w, s in zip(mt_split, spaces_after)]) for mt_split, refs, spaces_after in fixer.split_words(mt_words, mt_spaces)]\n",
    "    replacements = [\n",
    "        [join_if_tuple(w) + s for w, s in zip(mt_split, spaces_after)]\n",
    "        for mt_split, refs, spaces_after in fixer.split_words(mt_words, line_spaces, cutoff=3)\n",
    "    ]\n",
    "    replacements = [\"\".join(words) for words in replacements]\n",
    "    best = lm_score(mt_orig, replacements, lm=language_model)\n",
    "    corrected_texts_test.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_texts_test1 = [lmr(t, word=\" ς \", replacements=[\"ς \", \" \", \" σ \", \" σ\", \" ὡς \"], lm=language_model) for t in corrected_texts_test]\n",
    "ct_mod_test = corrected_texts_test1\n",
    "for w in unknown_words:\n",
    "    ct_mod = [\n",
    "        lmr(t, word=f\" {w} \", replacements=[f\"{w} \", f\" {w}\", \" \"], lm=language_model)\n",
    "        for t in ct_mod_test\n",
    "    ]\n",
    "# corrected_texts_test2 = [\n",
    "#     lmr(t, word=\" δ \", replacements=[\"δ \", \" δ\", \" \"], lm=language_model)\n",
    "#     for t in corrected_texts_test1\n",
    "# ]\n",
    "# corrected_texts_test2 = [postprocess_sigmas(t) for t in corrected_texts_test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Transcriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>105 Bodleian-Library-MS-Barocci-59_00085_fol-4...</td>\n",
       "      <td>τὲ πρμον ην τὴν αν θησιν τὴν σρ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               ImageID  \\\n",
       "110  105 Bodleian-Library-MS-Barocci-59_00085_fol-4...   \n",
       "\n",
       "                      Transcriptions  \n",
       "110  τὲ πρμον ην τὴν αν θησιν τὴν σρ  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(\n",
    "    zip(test_df.IMAGE_PATH, ct_mod_test),\n",
    "    columns=[\"ImageID\", \"Transcriptions\"]\n",
    ")\n",
    "submission.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mAn unexpected error occured!\u001b[0m\n",
      "HTTPSConnectionPool(host='www.aicrowd.com', port=443): Max retries exceeded with url: /api/v1/api_user (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa22ac1c410>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "To get more information, you can run this command with -v.\n",
      "To increase level of verbosity, you can go upto -vvvvv\n"
     ]
    }
   ],
   "source": [
    "%aicrowd submission create -c htrec-2022 -f submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_spaces = test_df.SYSTEM_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values\n",
    "#train_spaces1 = train_df.SYSTEM_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values\n",
    "#train_spaces2 = train_df.HUMAN_TRANSCRIPTION.apply(lambda x: [w for w in word_regex2.split(x) if word_regex.match(w)]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#Counter([x for xs in test_spaces for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter([x for xs in train_spaces1 for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([x for xs in train_spaces2 for xss in xs for x in xss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d632e93423067b2b9b1b8d52846f85c868da433699adad534ad2cc6673775271"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
